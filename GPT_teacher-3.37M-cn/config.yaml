model:
  n_layer: 4         # 模型的Transformer层数，决定了模型的深度。这个小模型使用4层，平衡了参数量和性能。
  n_head: 4          # 注意力头的数量，每个头可以学习不同的语义表示。4个注意力头适合小参数量模型。每个头的维度为64（256/4）。
  n_embd: 256        # 嵌入向量的维数，决定输入的维度。这个小模型使用256维的嵌入向量。
  seq_len: 128       # 序列最大长度，模型能处理的最大token数量。设为128是为了在CPU上高效训练。
  dropout: 0.0       # 丢弃率，用于防止过拟合。决定模型训练时是否进行dropout。这个小模型不使用dropout。
training:
  batch_size: 16     # 批次大小，决定每次训练的样本数量。这个小模型使用16个样本进行训练。
  micro_batch: 4     # 实际每次前向传播的批次大小，用于梯度累积。微批次大小，每个批次进一步分为4个微批次进行训练。这个小模型使用4个微批次。
  lr: 0.0003         # 学习率，决定模型训练时参数的更新速度。这个小模型使用0.0003的学习率。
  weight_decay: 0.01 # 权重衰减，一种正则化方法，防止模型过拟合。权重衰减决定模型训练时参数的更新大小。这个小模型使用0.01的权重衰减。
  max_steps: 2000    # 最大训练步数，决定模型训练的轮数。这个小模型使用2000个训练步数。
  warmup_steps: 5    # 预热步数，决定模型训练时参数的预热数量。这个小模型使用5个预热步数。
  eval_interval: 20  # 评估间隔，决定模型训练时评估的间隔。这个小模型使用20个训练步数间隔评估模型性能。
  save_dir: checkpoints # 模型保存目录。
  seed: 42           # 随机数种子，决定模型训练时参数的初始化。这个小模型使用42作为随机数种子。
data:
  train_path: data/train.jsonl      # 训练数据路径。
  val_path: data/val.jsonl          # 验证数据路径。
  format: instruct                  # 数据格式，这里使用instruct格式，包含prompt和completion字段。
tokenizer:
  type: hf_tokenizers              # 分词器类型，这里使用hf_tokenizers，即Hugging Face的分词器。
  path: tokenizer/tokenizer.json   # 分词器路径，这里使用tokenizer/tokenizer.json。
